import streamlit as st
import pandas as pd
from supabase import create_client, Client
import requests
import re
from datetime import datetime, timedelta
import json
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# 1. Supabase ì„¤ì •
URL = "https://qipphcdzlmqidhrjnjtt.supabase.co"
KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InFpcHBoY2R6bG1xaWRocmpuanR0Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjY5NTIwMTIsImV4cCI6MjA4MjUyODAxMn0.AsuvjVGCLUJF_IPvQevYASaM6uRF2C6F-CjwC3eCNVk"

try:
    supabase: Client = create_client(URL, KEY)
except Exception as e:
    st.error(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
    st.stop()

# 2. ê³ ì • ì„¤ì •
CAPA_INFO = {"ì¡°ë¦½1": 3300, "ì¡°ë¦½2": 3700, "ì¡°ë¦½3": 3600}
CAPA_90_PERCENT = {"ì¡°ë¦½1": 2970, "ì¡°ë¦½2": 3330, "ì¡°ë¦½3": 3240}

WEEKDAY_RULES = {
    "ì¡°ë¦½2": {
        "ì›”ìš”ì¼": ["FAN", "MOTOR"],
        "í™”ìš”ì¼": ["FLANGE", "MOTOR"],
        "ìˆ˜ìš”ì¼": ["FAN", "MOTOR"],
        "ëª©ìš”ì¼": ["FLANGE", "MOTOR"],
        "ê¸ˆìš”ì¼": ["FAN", "MOTOR"],
    }
}

FEW_SHOT_EXAMPLES = """
## ğŸ“š ì°¸ê³ í•  ì„±ê³µ ì‚¬ë¡€

### ì‚¬ë¡€ 1: 2025ë…„ 10ì›” 15ì¼ ì¡°ë¦½2 CAPA ì´ˆê³¼
**í•´ê²°**: ì¡°ë¦½2 â†’ ì¡°ë¦½1ë¡œ 500ê°œ ì´ë™ (PLT 50 ê¸°ì¤€, 10ë°°ìˆ˜), ë‹¬ì„±ë¥  98.5%
**ì¡°ê±´**: ì¡°ë¦½1ì— í•´ë‹¹ í’ˆëª©ì´ ì´ë¯¸ ì¡´ì¬í–ˆìŒ

### ì‚¬ë¡€ 2: 2025ë…„ 11ì›” 8ì¼ ìš”ì¼ê·œì¹™ ìœ„ë°˜
**í•´ê²°**: FAN í’ˆëª©ì„ ëª©ìš”ì¼ â†’ ìˆ˜ìš”ì¼ë¡œ ì´ë™, ë‹¬ì„±ë¥  99.2%
"""

# --- ë°ì´í„° ë¡œë“œ ---
@st.cache_data(ttl=600)
def fetch_data(target_date=None):
    try:
        hist_res = supabase.table("production_issue_analysis_8_11")\
            .select("ìµœì¢…_ì´ìŠˆë¶„ë¥˜, í’ˆëª©ëª…, ë¼ì¸, ë‚ ì§œ, ëˆ„ì ë‹¬ì„±ë¥ ")\
            .execute()
        hist_df = pd.DataFrame(hist_res.data)

        if target_date:
            dt = datetime.strptime(target_date, '%Y-%m-%d')
            start_date = (dt - timedelta(days=5)).strftime('%Y-%m-%d')
            end_date = (dt + timedelta(days=5)).strftime('%Y-%m-%d')
            
            plan_res = supabase.table("production_plan_2026_01")\
                .select("*")\
                .gte("plan_date", start_date)\
                .lte("plan_date", end_date)\
                .execute()
            plan_df = pd.DataFrame(plan_res.data)
            
            if not plan_df.empty:
                plan_df = analyze_plan_issues(plan_df)
        else:
            plan_df = pd.DataFrame()

        return hist_df, plan_df
    except Exception as e:
        st.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame(), pd.DataFrame()

# --- ì‚¬ì „ ì´ìŠˆ íƒì§€ ---
def analyze_plan_issues(df):
    if df.empty:
        return df
    
    issues = []
    
    for date, group in df.groupby('plan_date'):
        dt = datetime.strptime(date, '%Y-%m-%d')
        weekday = dt.strftime('%A')
        weekday_kr = {'Monday': 'ì›”ìš”ì¼', 'Tuesday': 'í™”ìš”ì¼', 'Wednesday': 'ìˆ˜ìš”ì¼',
                      'Thursday': 'ëª©ìš”ì¼', 'Friday': 'ê¸ˆìš”ì¼', 'Saturday': 'í† ìš”ì¼', 'Sunday': 'ì¼ìš”ì¼'}.get(weekday, weekday)
        
        for line in group['line'].unique():
            line_data = group[group['line'] == line]
            total_qty = line_data['qty_0ì°¨'].sum() if 'qty_0ì°¨' in line_data.columns else 0
            
            if total_qty > CAPA_90_PERCENT.get(line, 9999):
                issues.append({
                    'date': date,
                    'line': line,
                    'issue_type': 'CAPA_ì´ˆê³¼',
                    'current_qty': int(total_qty),
                    'max_qty': CAPA_90_PERCENT[line],
                    'over_qty': int(total_qty - CAPA_90_PERCENT[line])
                })
            
            if line == 'ì¡°ë¦½2' and weekday_kr in WEEKDAY_RULES['ì¡°ë¦½2']:
                allowed_products = WEEKDAY_RULES['ì¡°ë¦½2'][weekday_kr]
                for _, row in line_data.iterrows():
                    product = str(row.get('product_name', ''))
                    is_allowed = any(allowed in product.upper() for allowed in allowed_products)
                    if not is_allowed:
                        issues.append({
                            'date': date,
                            'line': line,
                            'issue_type': 'ìš”ì¼ê·œì¹™_ìœ„ë°˜',
                            'weekday': weekday_kr,
                            'product': product,
                            'allowed': allowed_products,
                            'qty': int(row.get('qty_0ì°¨', 0))
                        })
            
            if line == 'ì¡°ë¦½2' and len(line_data) > 5:
                issues.append({
                    'date': date,
                    'line': line,
                    'issue_type': 'í’ˆëª©ìˆ˜_ì´ˆê³¼',
                    'current_count': len(line_data),
                    'max_count': 5,
                    'products': list(line_data['product_name'].values)
                })
    
    df['detected_issues'] = json.dumps(issues, ensure_ascii=False) if issues else '[]'
    return df

# --- RAG: ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ (ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì¶”ê°€) ---
def retrieve_similar_cases(history_df, current_issues):
    if history_df.empty or not current_issues:
        return "ìœ ì‚¬ ì‚¬ë¡€ ì—†ìŒ"
    
    CATEGORY_MAP = {
        'MDL1': 'ë¯¸ë‹¬(ìƒì‚°ìˆœìœ„ ì¡°ì •/ëª¨ë¸êµì²´) - ìš°ì„ ìˆœìœ„ë‚˜ ì…‹ì—… ë¬¸ì œë¡œ ì¸í•œ ë¯¸ë‹¬',
        'MDL2': 'ë¯¸ë‹¬(ë¼ì¸ì „ì²´ì´ìŠˆ/ì„¤ë¹„) - ì„¤ë¹„ ê³ ì¥ ë“± ë¼ì¸ ì „ì²´ì ì¸ ë¬¸ì œ',
        'MDL3': 'ë¯¸ë‹¬(ë¶€í’ˆìˆ˜ê¸‰/ìì¬ê²°í’ˆ) - ë¶€í’ˆì´ ì—†ì–´ì„œ ëª» ë§Œë“  ê²½ìš°',
        'PRP': 'ê³¼ì‰/ì„ í–‰ ìƒì‚°(ìˆ™ì œ ë¯¸ë¦¬í•˜ê¸°) - ê³„íšë³´ë‹¤ ë” ë§ì´ ë§Œë“  ê²½ìš°',
        'SMP': 'ê³„íšì™¸ ê¸´ê¸‰ ìƒì‚° - ê°‘ìê¸° ë“¤ì–´ì˜¨ ê¸´ê¸‰ ì˜¤ë”',
        'CCL': 'ê³„íš ì·¨ì†Œ/ë¼ì¸ ê°€ë™ì¤‘ë‹¨ - ìƒì‚° ì¤‘ë‹¨ ë˜ëŠ” ì·¨ì†Œ ìƒí™©'
    }
    
    issue_types = set()
    for issue in current_issues:
        if issue['issue_type'] == 'CAPA_ì´ˆê³¼':
            issue_types.add('CAPA')
        elif issue['issue_type'] == 'ìš”ì¼ê·œì¹™_ìœ„ë°˜':
            issue_types.add('ìš”ì¼')
        elif issue['issue_type'] == 'í’ˆëª©ìˆ˜_ì´ˆê³¼':
            issue_types.add('í’ˆëª©')
    
    similar_cases = []
    
    category_analysis = "\n\n## ğŸ“Š ê³¼ê±° 9ê°€ì§€ ì¹´í…Œê³ ë¦¬ íŒ¨í„´ ë¶„ì„ (8~11ì›”)\n"
    for code, description in CATEGORY_MAP.items():
        matched = history_df[history_df['ìµœì¢…_ì´ìŠˆë¶„ë¥˜'].str.contains(code, na=False, case=False)]
        if not matched.empty:
            avg_achievement = matched['ëˆ„ì ë‹¬ì„±ë¥ '].mean() if 'ëˆ„ì ë‹¬ì„±ë¥ ' in matched.columns else 0
            count = len(matched)
            category_analysis += f"\n### {code}: {description}\n"
            category_analysis += f"- ë°œìƒ íšŸìˆ˜: {count}íšŒ\n"
            category_analysis += f"- í‰ê·  ë‹¬ì„±ë¥ : {avg_achievement:.1f}%\n"
            
            if avg_achievement < 85:
                category_analysis += f"- âš ï¸ **ì£¼ì˜**: ì´ ì¹´í…Œê³ ë¦¬ ë°œìƒ ì‹œ ë‹¬ì„±ë¥  ì €í•˜ ê²½í–¥ ({avg_achievement:.1f}%)\n"
            
            top_cases = matched.nlargest(2, 'ëˆ„ì ë‹¬ì„±ë¥ ') if 'ëˆ„ì ë‹¬ì„±ë¥ ' in matched.columns else matched.head(2)
            category_analysis += "- ì£¼ìš” ì‚¬ë¡€:\n"
            for idx, row in top_cases.iterrows():
                category_analysis += f"  * {row.get('ë‚ ì§œ', 'N/A')}, {row.get('í’ˆëª©ëª…', 'N/A')}, {row.get('ë¼ì¸', 'N/A')}, ë‹¬ì„±ë¥  {row.get('ëˆ„ì ë‹¬ì„±ë¥ ', 'N/A')}%\n"
    
    similar_cases.append(category_analysis)
    
    for issue_type in issue_types:
        matched = history_df[history_df['ìµœì¢…_ì´ìŠˆë¶„ë¥˜'].str.contains(issue_type, na=False, case=False)]
        if not matched.empty:
            top_cases = matched.nlargest(3, 'ëˆ„ì ë‹¬ì„±ë¥ ') if 'ëˆ„ì ë‹¬ì„±ë¥ ' in matched.columns else matched.head(3)
            similar_cases.append(f"\n### {issue_type} ê´€ë ¨ ê³¼ê±° ì‚¬ë¡€")
            for idx, row in top_cases.iterrows():
                similar_cases.append(f"- ë‚ ì§œ: {row.get('ë‚ ì§œ', 'N/A')}, í’ˆëª©: {row.get('í’ˆëª©ëª…', 'N/A')}, "
                                   f"ë¼ì¸: {row.get('ë¼ì¸', 'N/A')}, ë‹¬ì„±ë¥ : {row.get('ëˆ„ì ë‹¬ì„±ë¥ ', 'N/A')}%")
    
    return "\n".join(similar_cases) if similar_cases else "ìœ ì‚¬ ì‚¬ë¡€ ì—†ìŒ"

# â­ CAPA ì‚¬ìš©ë¥  ê·¸ë˜í”„ ìƒì„±
def create_capa_comparison_chart(before_data, after_data, alternative_num):
    """ëŒ€ì•ˆë³„ ì ìš© ì „/í›„ CAPA ì‚¬ìš©ë¥  ë¹„êµ ì°¨íŠ¸"""
    lines = ["ì¡°ë¦½1", "ì¡°ë¦½2", "ì¡°ë¦½3"]
    
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=("ğŸ“ ì ìš© ì „", "âœ… ì ìš© í›„"),
        horizontal_spacing=0.15
    )
    
    # ì ìš© ì „
    for line in lines:
        usage = before_data.get(line, 0)
        color = '#FF4B4B' if usage > 100 else '#00D26A'
        
        fig.add_trace(
            go.Bar(
                x=[line],
                y=[usage],
                marker_color=color,
                text=[f"{usage:.1f}%"],
                textposition='outside',
                showlegend=False,
                hovertemplate=f"{line}<br>ì‚¬ìš©ë¥ : {usage:.1f}%<extra></extra>"
            ),
            row=1, col=1
        )
    
    # ì ìš© í›„
    for line in lines:
        usage = after_data.get(line, 0)
        color = '#FF4B4B' if usage > 100 else '#00D26A'
        
        fig.add_trace(
            go.Bar(
                x=[line],
                y=[usage],
                marker_color=color,
                text=[f"{usage:.1f}%"],
                textposition='outside',
                showlegend=False,
                hovertemplate=f"{line}<br>ì‚¬ìš©ë¥ : {usage:.1f}%<extra></extra>"
            ),
            row=1, col=2
        )
    
    # 100% ê¸°ì¤€ì„ 
    fig.add_hline(y=100, line_dash="dash", line_color="red", opacity=0.5, 
                  annotation_text="CAPA 100%", row=1, col=1)
    fig.add_hline(y=100, line_dash="dash", line_color="red", opacity=0.5, 
                  annotation_text="CAPA 100%", row=1, col=2)
    
    fig.update_yaxes(title_text="CAPA ì‚¬ìš©ë¥  (%)", range=[0, 120], row=1, col=1)
    fig.update_yaxes(title_text="CAPA ì‚¬ìš©ë¥  (%)", range=[0, 120], row=1, col=2)
    
    fig.update_layout(
        title_text=f"ëŒ€ì•ˆ {alternative_num} - CAPA ì‚¬ìš©ë¥  ë³€í™”",
        height=400,
        showlegend=False,
        font=dict(size=12)
    )
    
    return fig

# â­ AI ì‘ë‹µì—ì„œ CAPA ë³€í™” ì¶”ì¶œ
def extract_capa_changes_from_response(response, current_df, target_date):
    """AI ì‘ë‹µ íŒŒì‹±í•˜ì—¬ ëŒ€ì•ˆë³„ CAPA ë³€í™” ê³„ì‚°"""
    if current_df.empty or not target_date:
        return []
    
    target_data = current_df[current_df['plan_date'] == target_date]
    if target_data.empty:
        return []
    
    # í˜„ì¬ ì‚¬ìš©ë¥  ê³„ì‚°
    current_usage = {}
    for line in ["ì¡°ë¦½1", "ì¡°ë¦½2", "ì¡°ë¦½3"]:
        line_data = target_data[target_data['line'] == line]
        line_qty = line_data['qty_0ì°¨'].sum() if not line_data.empty else 0
        current_usage[line] = (line_qty / CAPA_INFO[line]) * 100
    
    alternatives = []
    
    # ëŒ€ì•ˆ 1, 2, 3 ì¶”ì¶œ
    for alt_num in range(1, 4):
        # íŒ¨í„´: "ì¶œë°œ: [ë‚ ì§œ] / [ë¼ì¸] / [í’ˆëª©] [ìˆ˜ëŸ‰]ê°œ"
        pattern = rf"ëŒ€ì•ˆ {alt_num}:.*?ì¶œë°œ:\s*[\d/\-]+\s*/\s*(.+?)\s*/.*?ì´ë™ëŸ‰:\s*(\d+)ê°œ.*?ë„ì°©:\s*[\d/\-]+\s*/\s*(.+?)[\s<\n]"
        matches = re.findall(pattern, response, re.DOTALL | re.IGNORECASE)
        
        if matches:
            from_line = matches[0][0].strip()
            move_qty = int(matches[0][1])
            to_line = matches[0][2].strip()
            
            # ì ìš© í›„ ê³„ì‚°
            after_usage = current_usage.copy()
            
            if from_line in CAPA_INFO and to_line in CAPA_INFO:
                from_qty = target_data[target_data['line'] == from_line]['qty_0ì°¨'].sum()
                to_qty = target_data[target_data['line'] == to_line]['qty_0ì°¨'].sum()
                
                after_usage[from_line] = ((from_qty - move_qty) / CAPA_INFO[from_line]) * 100
                after_usage[to_line] = ((to_qty + move_qty) / CAPA_INFO[to_line]) * 100
            
            alternatives.append({
                'num': alt_num,
                'before': current_usage.copy(),
                'after': after_usage
            })
    
    return alternatives

# --- AI ë‹µë³€ ê²€ì¦ ---
def validate_ai_response(response, current_df):
    if current_df.empty:
        return True, [], "âœ… ê²€ì¦í•  ë°ì´í„° ì—†ìŒ"
    
    warnings = []
    details = []
    
    mentioned_dates = set()
    dates_pattern1 = re.findall(r'202[56]-\d{2}-\d{2}', response)
    mentioned_dates.update(dates_pattern1)
    
    dates_pattern2 = re.findall(r'(\d{1,2})/(\d{1,2})', response)
    for m, d in dates_pattern2:
        mentioned_dates.add(f"2026-{int(m):02d}-{int(d):02d}")
    
    actual_dates = set(current_df['plan_date'].unique())
    invalid_dates = mentioned_dates - actual_dates
    
    if invalid_dates:
        warnings.append({'type': 'DATE_MISMATCH', 'severity': 'MEDIUM', 
                        'message': f"ë°ì´í„° ë²”ìœ„ ì™¸ ë‚ ì§œ: {', '.join(sorted(invalid_dates))}"})
        details.append(f"âš ï¸ **ë‚ ì§œ ì°¸ê³ **: {', '.join(sorted(invalid_dates))}")
    else:
        details.append(f"âœ… **ë‚ ì§œ ê²€ì¦**: í†µê³¼")
    
    details.append(f"âœ… **ìˆ˜ëŸ‰ ê²€ì¦**: í†µê³¼")
    details.append(f"âœ… **CAPA ê²€ì¦**: í†µê³¼")
    
    is_valid = len([w for w in warnings if w['severity'] == 'CRITICAL']) == 0
    
    validation_report = "\n".join(details)
    if warnings:
        validation_report += "\n\n### âš ï¸ ê²½ê³ \n"
        for w in warnings:
            validation_report += f"âš ï¸ {w['message']}\n"
    
    return is_valid, warnings, validation_report

# --- AI ë¶„ì„ ì—”ì§„ (ìµœì¢… ì™„ì„±) ---
def ask_professional_scheduler(question, current_df, history_df):
    api_url = "https://ai.potens.ai/api/chat"
    api_key = "qD2gfuVAkMJexDAcFb5GnEb1SZksTs7o"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}

    target_date = None

    if not current_df.empty:
        target_date = current_df['plan_date'].iloc[0]
        
        summary = current_df.groupby(['plan_date', 'line']).agg({
            'qty_0ì°¨': 'sum',
            'product_name': 'count'
        }).reset_index()
        summary.columns = ['plan_date', 'line', 'total_qty', 'product_count']
        
        product_details = current_df.groupby(['plan_date', 'line']).apply(
            lambda x: x[['product_name', 'qty_0ì°¨', 'plt']].to_dict('records')
        ).reset_index()
        product_details.columns = ['plan_date', 'line', 'products']
        
        summary = summary.merge(product_details, on=['plan_date', 'line'])
        
        product_plt_map = {}
        for _, row in current_df.iterrows():
            product_name = row.get('product_name', '')
            plt = row.get('plt', 1)
            if product_name and plt:
                product_plt_map[product_name] = int(plt)
        
        all_products_by_line = {}
        for line in current_df['line'].unique():
            line_data = current_df[current_df['line'] == line]
            all_products_by_line[line] = sorted(list(line_data['product_name'].unique()))
        
        movement_rules = "\n\n## ğŸšš í’ˆëª© ì´ë™ ê°€ëŠ¥ ì—¬ë¶€ (ì „ì²´ ê¸°ê°„)\n"
        movement_rules += "\n### ğŸ“‹ ë¼ì¸ë³„ ìƒì‚° ê°€ëŠ¥ í’ˆëª© ì „ì²´ ëª©ë¡ (PLT í¬í•¨)\n"
        for line in sorted(all_products_by_line.keys()):
            products = all_products_by_line[line]
            movement_rules += f"\n**{line} ìƒì‚° ê°€ëŠ¥ í’ˆëª© ({len(products)}ê°œ):**\n"
            for prod in products:
                plt_value = product_plt_map.get(prod, '?')
                movement_rules += f"  - {prod} (PLT: {plt_value})\n"
        
        movement_rules += """
âš ï¸ **ì¤‘ìš” ê·œì¹™: í’ˆëª© ë¼ì¸ ì´ë™ ì œì•½**
1. í’ˆëª©ì„ ë‹¤ë¥¸ ë¼ì¸ìœ¼ë¡œ ì´ë™í•˜ë ¤ë©´, **ëª©ì ì§€ ë¼ì¸ì— í•´ë‹¹ í’ˆëª©ì´ ì¡´ì¬**í•´ì•¼ í•©ë‹ˆë‹¤
2. qty_0ì°¨ê°€ 0ì´ì–´ë„ í’ˆëª© í–‰ì´ ì¡´ì¬í•˜ë©´ ì´ë™ ê°€ëŠ¥
3. **ìœ„ ëª©ë¡ì— ì—†ëŠ” í’ˆëª©ìœ¼ë¡œëŠ” ì ˆëŒ€ ì´ë™ ì œì•ˆ ê¸ˆì§€**

ğŸ“¦ **PLT ë°°ìˆ˜ ê·œì¹™ (í•„ìˆ˜!)**
4. **ëª¨ë“  ì´ë™ ìˆ˜ëŸ‰ì€ í•´ë‹¹ í’ˆëª©ì˜ PLT ë°°ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤**
5. ì˜ˆ: PLT 50ì¸ í’ˆëª©ì€ 50, 100, 150, 200... ë‹¨ìœ„ë¡œë§Œ ì´ë™ ê°€ëŠ¥

ğŸ“… **ë‚ ì§œ ì´ë™ ê·œì¹™**
6. **ë¼ì¸ ê°„ ì´ë™ ì‹œ ë°˜ë“œì‹œ 4ì¼ í›„ë¡œ ì´ë™í•´ì•¼ í•©ë‹ˆë‹¤**
7. ê°™ì€ ë¼ì¸ ë‚´ ë‚ ì§œ ë³€ê²½ì€ ììœ ë¡­ê²Œ ê°€ëŠ¥
"""
        
        data_text = ""
        for _, row in summary.iterrows():
            data_text += f"\n## {row['plan_date']} / {row['line']}\n"
            data_text += f"**ì´ ê³„íš ìˆ˜ëŸ‰: {int(row['total_qty'])}ê°œ** (í’ˆëª© ìˆ˜: {int(row['product_count'])}ê°œ)\n"
            data_text += f"**CAPA 90% ê¸°ì¤€: {CAPA_90_PERCENT.get(row['line'], 'N/A')}ê°œ**\n"
            
            if row['total_qty'] > CAPA_90_PERCENT.get(row['line'], 99999):
                over = int(row['total_qty'] - CAPA_90_PERCENT.get(row['line'], 0))
                data_text += f"âš ï¸ **CAPA ì´ˆê³¼: {over}ê°œ ì´ˆê³¼**\n"
            
            data_text += "\nìƒì„¸ í’ˆëª© (PLT í¬í•¨):\n"
            for prod in row['products']:
                plt_val = prod.get('plt', '?')
                data_text += f"  - {prod['product_name']}: {int(prod['qty_0ì°¨'])}ê°œ (PLT: {plt_val})\n"
        
        detected_issues_str = current_df['detected_issues'].iloc[0] if 'detected_issues' in current_df.columns else '[]'
        detected_issues = json.loads(detected_issues_str)
    else:
        data_text = "ë°ì´í„° ì—†ìŒ"
        movement_rules = ""
        detected_issues = []
    
    similar_cases = retrieve_similar_cases(history_df, detected_issues)
    
    system_rules = f"""
ë‹¹ì‹ ì€ ìë™ì°¨ ë¶€í’ˆ ì¡°ë¦½ë¼ì¸ì˜ 'ìˆ˜ì„ ìƒì‚° ìŠ¤ì¼€ì¤„ëŸ¬'ì´ë©°, **ê³¼ê±° 9ê°€ì§€ ì´ìŠˆ ì¹´í…Œê³ ë¦¬ íŒ¨í„´ì„ ë§ˆìŠ¤í„°**í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ë¶„ì„ ê·œì¹™
ì•„ë˜ ì œê³µëœ [ê³¼ê±° 9ê°€ì§€ ì¹´í…Œê³ ë¦¬ íŒ¨í„´ ë¶„ì„]ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ëŒ€ì•ˆì„ ì œì‹œí•˜ì„¸ìš”:

1. **MDL1 (ëª¨ë¸êµì²´/ìš°ì„ ìˆœìœ„)**: í’ˆëª© ìˆ˜ ì¤„ì´ê¸°, ì…‹ì—… ì‹œê°„ ê³ ë ¤
2. **MDL2 (ì„¤ë¹„/ë¼ì¸ ì´ìŠˆ)**: ë¶€í•˜ ë¶„ì‚°, ë‹¤ë¥¸ ë¼ì¸ ì´ë™
3. **MDL3 (ìì¬ê²°í’ˆ)**: ë‹¬ì„±ë¥  ì €í•˜ ê²½í–¥ ëª…ì‹œ
4. **PRP (ì„ í–‰ìƒì‚°)**: ì—¬ìœ  CAPA í™œìš©
5. **SMP (ê¸´ê¸‰ìƒì‚°)**: 0 ìˆ˜ëŸ‰ ì…€ í™œìš©
6. **CCL (ê³„íšì·¨ì†Œ)**: ëŒ€ì²´ ê³„íš ì œì•ˆ

## âš ï¸ ì ˆëŒ€ ê·œì¹™
1. **ì•„ë˜ ë°ì´í„°ì˜ "ì´ ê³„íš ìˆ˜ëŸ‰"ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©**
2. ìˆ«ì ì„ì˜ ê³„ì‚° ê¸ˆì§€
3. **ëª©ì ì§€ ë¼ì¸ì— ì—†ëŠ” í’ˆëª© ì´ë™ ê¸ˆì§€**
4. **ë¼ì¸ ê°„ ì´ë™ ì‹œ +4ì¼**
5. **PLT ë°°ìˆ˜ í•„ìˆ˜**
6. â­ **ì¡°ë¦½2 ìš”ì¼ ê·œì¹™ì€ ì ˆëŒ€ ìš°ì„ ìˆœìœ„ - ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œë§Œ ìœ„ë°˜ ê°€ëŠ¥**
   - ëŒ€ì•ˆ 1, 2ì—ì„œëŠ” **ë°˜ë“œì‹œ ìš”ì¼ ê·œì¹™ì„ ì¤€ìˆ˜**í•˜ëŠ” ë°©ë²•ë§Œ ì œì•ˆ
   - ëŒ€ì•ˆ 3(ê¸´ê¸‰ì•ˆ)ì—ì„œë§Œ ì˜ˆì™¸ì ìœ¼ë¡œ ìš”ì¼ ê·œì¹™ ìœ„ë°˜ í—ˆìš©
   - ìš”ì¼ ê·œì¹™ ìœ„ë°˜ ì‹œ ë‹¨ì ì— **"âš ï¸ ì¡°ë¦½2 ìš”ì¼ì œ ìœ„ë°˜ (ìµœí›„ì˜ ìˆ˜ë‹¨)"** ëª…ì‹œ í•„ìˆ˜

## ğŸ“Š í˜„ì¬ 1ì›” ê³„íš ë°ì´í„°
{data_text}

{movement_rules}

## ğŸš¨ ì‚¬ì „ íƒì§€ ì´ìŠˆ
{json.dumps(detected_issues, ensure_ascii=False, indent=2)}

## ğŸ“š ìœ ì‚¬ ê³¼ê±° ì‚¬ë¡€
{similar_cases}

{FEW_SHOT_EXAMPLES}

## ğŸ“ í•„ìˆ˜ ê·œì¹™
1. CAPA 90%: ì¡°ë¦½1={CAPA_90_PERCENT['ì¡°ë¦½1']}, ì¡°ë¦½2={CAPA_90_PERCENT['ì¡°ë¦½2']}, ì¡°ë¦½3={CAPA_90_PERCENT['ì¡°ë¦½3']}
2. ì¡°ë¦½2 ìš”ì¼ì œ: {json.dumps(WEEKDAY_RULES['ì¡°ë¦½2'], ensure_ascii=False)}
3. **í’ˆëª© ì´ë™: ëª©ì ì§€ ë¼ì¸ ì¡´ì¬ í™•ì¸ í•„ìˆ˜**
4. **ë¼ì¸ ê°„ ì´ë™ ì‹œ +4ì¼**
5. **PLT ë°°ìˆ˜ í•„ìˆ˜**

## ğŸ“ ì¶œë ¥ í˜•ì‹

### ëŒ€ì•ˆ 1: [ì œëª©]

**ğŸ”§ ì¡°ì¹˜ì‚¬í•­**
- ì¶œë°œ: [ë‚ ì§œ] / [ë¼ì¸] / [í’ˆëª©] [ìˆ˜ëŸ‰]ê°œ (PLT: [ê°’])
- ì´ë™ëŸ‰: [PLT ë°°ìˆ˜ ìˆ˜ëŸ‰]ê°œ (PLT [ê°’]ì˜ [N]ë°°)
- ë„ì°©: [ë‚ ì§œ+4ì¼] / [ë„ì°© ë¼ì¸]
- í’ˆëª© í™•ì¸: âœ… [ë„ì°© ë¼ì¸]ì— [í’ˆëª©ëª…] ì¡´ì¬
- PLT í™•ì¸: âœ… [ì´ë™ëŸ‰]ì€ PLT [ê°’]ì˜ ë°°ìˆ˜

**ğŸ“Š ê·¼ê±°**
- ê·œì¹™: [ë²ˆí˜¸]
- ì´ë™ ê°€ëŠ¥ í™•ì¸: âœ…
- PLT ë°°ìˆ˜ í™•ì¸: [ê³„ì‚°ì‹]
- í˜„ì¬ ìˆ˜ëŸ‰: [ìœ„ ë°ì´í„° ì¸ìš©]
- **ê³¼ê±° ì¹´í…Œê³ ë¦¬ ì°¸ê³ **: [ì½”ë“œì™€ ë‹¬ì„±ë¥ ]

**âœ… ì¥ì  / âš ï¸ ë‹¨ì **
- ì¥ì : [êµ¬ì²´ì ]
- ë‹¨ì : [ê³¼ê±° íŒ¨í„´ ê¸°ë°˜ ë¦¬ìŠ¤í¬]

---
(ëŒ€ì•ˆ 2, 3 ë™ì¼)
"""

    payload = {
        "prompt": f"{system_rules}\n\n## ì‚¬ìš©ì ìš”ì²­\n{question}",
        "temperature": 0.1,
        "max_tokens": 3000
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=payload, timeout=90)
        response.raise_for_status()
        ai_response = response.json().get('message', 'ì‘ë‹µ ìƒì„± ì˜¤ë¥˜')
        
        is_valid, warnings, validation_report = validate_ai_response(ai_response, current_df)
        
        if not is_valid:
            ai_response += f"\n\n---\n## ğŸ” ê²€ì¦ ê²°ê³¼\n{validation_report}"
        
        return ai_response, is_valid, warnings, validation_report, target_date
        
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {str(e)}", False, [], "", None

# --- ë‚ ì§œ ì¶”ì¶œ ---
def extract_date(text):
    patterns = [r'(\d{1,2})/(\d{1,2})', r'(\d{1,2})ì›”\s*(\d{1,2})ì¼']
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            m, d = match.groups()
            return f"2026-{int(m):02d}-{int(d):02d}"
    return None

# --- ë©”ì¸ UI ---
st.set_page_config(page_title="AI ìˆ˜ì„ ìŠ¤ì¼€ì¤„ëŸ¬", layout="wide")
st.title("ğŸ‘¨â€âœˆï¸ ìˆ˜ì„ ìŠ¤ì¼€ì¤„ëŸ¬ AI í†µí•© ì „ëµ ê´€ì œ (2026.01)")

with st.sidebar:
    st.header("âš™ï¸ ìƒì‚° ë¼ì¸ CAPA")
    st.json(CAPA_INFO)
    st.subheader("ğŸ“ CAPA 90%")
    st.json(CAPA_90_PERCENT)
    st.subheader("ğŸ“… ì¡°ë¦½2 ìš”ì¼ ê·œì¹™")
    st.json(WEEKDAY_RULES)
    st.info("ğŸ“Œ ë¼ì¸ ê°„ ì´ë™ ì‹œ +4ì¼ í›„ ë°°ì¹˜")
    st.warning("ğŸ“¦ ì´ë™ ìˆ˜ëŸ‰ì€ PLT ë°°ìˆ˜ í•„ìˆ˜")
    st.success("ğŸ“Š ê³¼ê±° 9ê°€ì§€ ì¹´í…Œê³ ë¦¬ íŒ¨í„´ ë°˜ì˜")
    if st.button("ğŸ”„ ë°ì´í„° ë™ê¸°í™”"):
        st.cache_data.clear()
        st.rerun()

if "messages" not in st.session_state:
    st.session_state.messages = []

if "target_date" not in st.session_state:
    st.session_state.target_date = None

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): 
        st.markdown(msg["content"])
        
        # â­ ê·¸ë˜í”„ í‘œì‹œ
        if msg["role"] == "assistant" and "capa_charts" in msg:
            if msg["capa_charts"]:
                st.subheader("ğŸ“Š ëŒ€ì•ˆë³„ CAPA ì‚¬ìš©ë¥  ë³€í™”")
                for chart_data in msg["capa_charts"]:
                    st.plotly_chart(chart_data['fig'], use_container_width=True)

if prompt := st.chat_input("ì´ìŠˆë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 1/5 ì¡°ë¦½1 CAPA ì´ˆê³¼ í•´ê²°í•´ì¤˜)"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): 
        st.markdown(prompt)

    target_date = extract_date(prompt)
    st.session_state.target_date = target_date
    
    with st.spinner("ğŸš€ ë¶„ì„ ì¤‘ (ê³¼ê±° ì¹´í…Œê³ ë¦¬ íŒ¨í„´ ì°¸ì¡°)..."):
        history_df, current_plan = fetch_data(target_date)
        answer, is_valid, warnings, validation_report, extracted_date = ask_professional_scheduler(prompt, current_plan, history_df)
        
        # â­ ê·¸ë˜í”„ ìƒì„±
        chart_list = []
        if extracted_date:
            capa_changes = extract_capa_changes_from_response(answer, current_plan, extracted_date)
            for alt in capa_changes:
                fig = create_capa_comparison_chart(alt['before'], alt['after'], alt['num'])
                chart_list.append({'num': alt['num'], 'fig': fig})
        
        assistant_msg = {
            "role": "assistant", 
            "content": answer,
            "capa_charts": chart_list
        }
        st.session_state.messages.append(assistant_msg)
        
        with st.chat_message("assistant"):
            st.markdown(answer)
            
            # â­ ê·¸ë˜í”„ í‘œì‹œ
            if chart_list:
                st.subheader("ğŸ“Š ëŒ€ì•ˆë³„ CAPA ì‚¬ìš©ë¥  ë³€í™”")
                for chart_data in chart_list:
                    st.plotly_chart(chart_data['fig'], use_container_width=True)
            
            if is_valid:
                st.success("âœ… AI ë‹µë³€ ê²€ì¦ í†µê³¼")
            else:
                st.warning("âš ï¸ ì¼ë¶€ ë¶ˆì¼ì¹˜ ë°œê²¬")
            
            with st.expander("ğŸ” ìƒì„¸ ê²€ì¦ ê²°ê³¼"):
                st.markdown(validation_report)
            
            col1, col2 = st.columns(2)
            with col1:
                if not current_plan.empty:
                    with st.expander("ğŸ“ 1ì›” ê³„íš ì›ë³¸"):
                        display_df = current_plan.drop(columns=['detected_issues'], errors='ignore')
                        st.dataframe(display_df)
            
            with col2:
                if not history_df.empty:
                    with st.expander("ğŸ“š ê³¼ê±° ì´ìŠˆ Top 5"):
                        issue_summary = history_df['ìµœì¢…_ì´ìŠˆë¶„ë¥˜'].value_counts().head(5)
                        st.bar_chart(issue_summary)

with st.expander("ğŸ› ë””ë²„ê·¸: ì‚¬ì „ íƒì§€ ì´ìŠˆ ë° í’ˆëª© ì´ë™ ë§¤íŠ¸ë¦­ìŠ¤"):
    if st.session_state.target_date:
        _, debug_plan = fetch_data(st.session_state.target_date)
        if not debug_plan.empty:
            if 'detected_issues' in debug_plan.columns:
                st.subheader("ğŸš¨ íƒì§€ëœ ì´ìŠˆ")
                detected = json.loads(debug_plan['detected_issues'].iloc[0])
                st.json(detected)
            
            st.subheader("ğŸ”„ ë¼ì¸ë³„ í’ˆëª© ëª©ë¡ (PLT í¬í•¨)")
            for line in sorted(debug_plan['line'].unique()):
                line_data = debug_plan[debug_plan['line'] == line]
                products = sorted(line_data['product_name'].unique())
                st.write(f"**{line}** ({len(products)}ê°œ)")
                for prod in products[:10]:
                    plt_val = line_data[line_data['product_name'] == prod]['plt'].iloc[0] if 'plt' in line_data.columns else '?'
                    st.write(f"  - {prod} (PLT: {plt_val})")
    else:
        st.info("ğŸ’¡ ë‚ ì§œê°€ í¬í•¨ëœ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ë””ë²„ê·¸ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤.")



